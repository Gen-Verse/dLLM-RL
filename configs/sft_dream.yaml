wandb:
  entity: null
  resume: 'auto'


experiment:
    project: "sft_dream"
    num_node: 4


model:
    pretrained_model: "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-Coder-v0-Instruct-7B/snapshots/5d9e88c723af9045f362748b5284bdf43d9c501e"
    optimized_name: "optimized0"

# "/data_storage/wyj/DLM/DLLM-RL/sft_dream/ckpt/optimized0"
# "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-Coder-v0-Instruct-7B/snapshots/5d9e88c723af9045f362748b5284bdf43d9c501e"
# "/data_storage/wyj/DLM/mmada/dream_training/ckpt/optimized2"
# "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-v0-Instruct-7B/snapshots/cafd9f3a3442d87c6d976a6f2c9d039e46a421be"
# "/data_storage/wyj/systems/huggingface/hub/models--apple--DiffuCoder-7B-Instruct/snapshots/b7b61c3ec85302700c65f10972cf267c02f88443"

dataset:
    optimization_data: "sft_data_code" # "sft_openr1math_dream"

training:
    gradient_checkpointing_enable: False
    gradient_accumulation_steps: 1
    batch_size_lm: 2
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    num_train_epochs: 2
    max_grad_norm: 1
    method: "ar" # "random_masking""semi-ar""ar"
    lower_p: 1.0
    upper_p: 1.0
    block_size: 16
    mask_times_per_sample: 35 # for random_masking
    post_num: null
    max_gen_length: 1024
    max_prompt_len: 784



optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 1e-5
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 0
        min_lr_scale: 1.0


