wandb:
  entity: null
  resume: 'auto'

experiment:
  project: "sft_lladav"
  num_node: 1

model:
  pretrained_model: "/abs/path/of/model"
  optimized_name: "optimized"

dataset:
  optimization_data: "sft_test"   # ./data/sft_test.json
  data_type: "mmu"
  image_root: "/base_dir/data/images"

training:
  gradient_checkpointing_enable: False
  gradient_accumulation_steps: 2
  batch_size_lm: 1
  mixed_precision: "bf16"
  enable_tf32: True
  seed: 10086
  num_train_epochs: 1
  max_grad_norm: 1.0
  method: "random_masking"     # "random_masking" or "semi-ar"
  lower_p: 0.1
  upper_p: 0.9
  block_size: 16               # for semi-ar
  mask_times_per_sample: 2     # for random_masking
  max_gen_length: 512

optimizer:
  name: adamw
  params:
    learning_rate: 1e-5
    scale_lr: False
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    epsilon: 1e-8

lr_scheduler:
  scheduler: "cosine"
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 0
    min_lr_scale: 1.0
