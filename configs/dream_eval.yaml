experiment:
    project: "dream_eval"
    num_node: 8 # the number of machines you have
    node_index: 0 # no need to change


# only need to be set when using multi-nodes
system:
    HTTP_PROXY: "http://100.68.168.184:3128"
    HF_HOME: "/data_storage/wyj/systems/huggingface"
    env_name: "CURE"
    envs_dir: "/data_storage/wyj/systems/envs"
    base_dir: "/data_storage/wyj/DLM/DLLM-RL"


model: "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-v0-Instruct-7B/snapshots/cafd9f3a3442d87c6d976a6f2c9d039e46a421be" # absolute path of your model
model_base: "dream" # set dream for Dream and Diffu-coder models

# "/data_storage/wyj/DLM/DLLM-RL/sft_dream/ckpt/optimized1"
# "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-Coder-v0-Instruct-7B/snapshots/5d9e88c723af9045f362748b5284bdf43d9c501e"
# /data_storage/wyj/systems/huggingface/hub/models--apple--DiffuCoder-7B-Instruct/snapshots/b7b61c3ec85302700c65f10972cf267c02f88443
# "/data_storage/wyj/systems/huggingface/hub/models--Dream-org--Dream-v0-Instruct-7B/snapshots/cafd9f3a3442d87c6d976a6f2c9d039e46a421be"
# "/data_storage/wyj/DLM/DLLM-RL/sft_dream/ckpt/optimized0"

dataset:
    eval_dataset: "LiveBench" #"MBPP""MATH500""GSM8K""AIME2024""GPQA""LiveCodeBench""HumanEval""LiveBench"
    data_type: "code" #"code""math""option"

execute:
    num_chunk: 128 # batch size of executing codes in coding eval tasks

rollout:
    num_response_per_task: 3
    temperature: 0.1
    alg_temp: 0
    pad_target_penalty: 1.0 # sometimes the pad token has very large logits, leads to early stop in inference. 1.0 means no penalty
    steps: 1024 # total steps of unmasking
    max_gen_length: 1024
    batch_size: 2
    top_p: 0.95
    top_k: 40
    remasking_strategy: "low_confidence_dynamic" #"low_confidence_static""low_confidence_dynamic"
    target: "confidence" # target to decide which tokens to unmask, eg. confidence, margin_confidence and neg_entropy
    dynamic_threshold: 0.95 # no use for "low_confidence_static"
    block_size: 4
    further_horizon: 128 # We find that performing the forward pass with a further horizon, eg. 128, (kv-cache sequence before current block, while doing full forward for currect [block_size + further_horizon] length sequence) yields competitive performance to running without KV-cache. Balances speed and performance.
    output_unmasking_history: False
    use_cache: True



