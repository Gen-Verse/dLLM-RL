wandb:
  entity: null
  resume: 'auto'


experiment:
    project: "sft_sdar"
    num_node: 8


model:
    pretrained_model: "/data_storage/wyj/DLM/DLLM-RL/sft_sdar/ckpt/optimized_long5"
    optimized_name: "optimized_long5"

# "/data_storage/wyj/systems/huggingface/hub/models--JetLM--SDAR-8B-Chat/snapshots/de54a3350737454faabc3ddb40b8119866cc6ebd"
# "/data_storage/wyj/systems/huggingface/hub/models--JetLM--SDAR-4B-Chat/snapshots/824f7ab513778bd93ce9f0f9039b9ee56b23c250"
# "/data_storage/wyj/systems/huggingface/hub/models--JetLM--SDAR-1.7B-Chat/snapshots/553d10acb7a3e09fec332c38cfd5610811f27776"
# "/data_storage/wyj/DLM/DLLM-RL/multinode_rl_sdar/ckpt/epoch-60"
# "/data_storage/wyj/DLM/DLLM-RL/sft_sdar/ckpt/optimized_long2"

dataset:
    optimization_data: "sft_openthoughts_code" # "sft_openr1math_sdar" "sft_data_long_sdar" "cc_longcot_2k" "sft_openthoughts"

training:
    gradient_checkpointing_enable: True
    gradient_accumulation_steps: 1
    batch_size_lm: 1
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    num_train_epochs: 1
    max_grad_norm: 1
    method: "semi-ar" # "semi-ar""trace"
    lower_p: 0.1
    upper_p: 0.9
    block_size: 4
    shrink: 1
    post_num: 0
    max_gen_length: 17000 # 17000 8024
    max_prompt_len: 784



optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 1e-5
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 0
        min_lr_scale: 1.0


