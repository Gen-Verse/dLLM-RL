wandb:
  entity: null
  resume: 'auto'



# only need to be set when using multi-nodes
system:
    HTTP_PROXY: "http://100.68.168.184:3128"
    HF_HOME: "/data_storage/wyj/systems/huggingface"
    env_name: "CURE"
    envs_dir: "/data_storage/wyj/systems/envs"
    base_dir: "/data_storage/wyj/DLM/DLLM-RL"


experiment:
    project: "multinode_rl_llada"
    function: "train"
    start_from_scratch: True
    total_step: 200
    save_every: 30
    eval_every: 1
    current_epoch: 1
    deepspeed_file: "8_node_8_gpus_deepspeed_zero3"
    num_node: 8
    node_index: 0

model:
    pretrained_model: "/data_storage/wyj/systems/huggingface/hub/models--Gen-Verse--MMaDA-8B-MixCoT/snapshots/3ee0085f0c42541f1134aae30482954451952406"
    optimized_name: "optimized2"
    model_base: "mmada"


# "/data_storage/wyj/systems/huggingface/hub/models--GSAI-ML--LLaDA-8B-Instruct/snapshots/9275bf8f5a5687507189baf4657e91c51b2be338"



dataset:
    train_dataset: "PrimeIntellect" #"MATH_train"
    optimization_data: "rl_data"
    data_type: "code" #"math"


rollout:
    num_task_per_step: 56
    num_response_per_task: 8
    temperature: 0.8
    steps: 1024
    max_gen_length: 1024
    batch_size: 2
    remasking_strategy: "low_confidence_static" #"low_confidence_static""low_confidence_dynamic"
    target: "confidence" # target to decide which tokens to unmask, eg. confidence, margin_confidence and neg_entropy
    dynamic_threshold: 0.95 # no use for "low_confidence_static"
    block_size: 32
    further_horizon: 128
    use_cache: True

execute:
    num_chunk: 128 # batch size of executing codes in coding eval tasks

training:
    gradient_accumulation_steps: 4
    batch_size_lm: 2
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    num_train_epochs: 1
    max_grad_norm: 1.0
    method: "TraceRL"
    lower_p: 0.1
    upper_p: 0.9
    shrink: 8
    post_num: 4
    mask_times_per_sample: 30
    max_gen_length: ${rollout.max_gen_length}
    max_prompt_len: 512
    eps: 0.20
    beta: 0.01
    use_kl_estimator_k3: True


optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 1e-6
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0
        epsilon: 1e-8


lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 0
        min_lr_scale: 1.0


evaluation:
    eval_dataset: "LiveCodeBench" #"MATH500"
    data_type: "code" #"math"
    num_response_per_task: 3
    temperature: 0.1
    steps: [1024, 1024]
    max_gen_length: 1024
    batch_size: 2
    remasking_strategy: ["low_confidence_static", "low_confidence_dynamic"] #"low_confidence_static""low_confidence_dynamic"
    target: "confidence" # target to decide which tokens to unmask, eg. confidence, margin_confidence and neg_entropy
    dynamic_threshold: 0.95 # no use for "low_confidence_static"
    block_size: 32
    further_horizon: 128
    use_cache: True